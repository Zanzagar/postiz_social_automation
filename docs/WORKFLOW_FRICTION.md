# Workflow Friction Log

| ID | Phase | Severity | Issue | Resolution |
|----|-------|----------|-------|------------|
| F1 | 1.1   | MEDIUM   | 5 language rules exist in subdirs (e.g. `.claude/rules/python/coding-standards.md`) but are NOT auto-loaded by Claude Code. Only top-level `.claude/rules/*.md` loaded. | By design — Claude Code only loads top-level rule files. Language rules need explicit invocation or should be moved to top level if always needed. |
| F2 | 1.1   | INFO     | No instincts directory created at bootstrap — `.claude/instincts/` is empty | Expected behavior — instincts are generated over time by continuous-learning skill. |
| F3 | 1.8   | INFO     | MCP count exceeds expectation — 4 MCPs loaded (task-master-ai, context7, playwright, ide) vs expected 2 | Extra MCPs from Claude Code IDE integration. Not a problem but adds ~15-20k tool definition tokens. |
| F4 | health | LOW      | sync-template.sh not available — /health check tries to run it | Template bootstrap via init-project.sh (copy mode) doesn't include sync-template.sh. Health check should handle missing script gracefully. |
| F5 | 1.7   | LOW      | tasks.json lives at `.taskmaster/tasks.json`, not `.taskmaster/tasks/tasks.json`. But `task-master list` creates/reads from `.taskmaster/tasks/tasks.json` path. Two files. | Task Master CLI creates its own path structure on first use. The `.taskmaster/tasks.json` from init is the seed; CLI creates `.taskmaster/tasks/` on first operation. |
| F6 | 3.1   | HIGH     | `task-master tags add` fails because `.taskmaster/tasks/` directory doesn't exist after init. CLI expects `.taskmaster/tasks/tasks.json`, but init only creates `.taskmaster/tasks.json`. | Manual fix: `mkdir -p .taskmaster/tasks && cp .taskmaster/tasks.json .taskmaster/tasks/tasks.json`. Template init-project.sh should create both paths. |
| F7 | 3.4   | HIGH     | `task-master analyze-complexity` fails because `.taskmaster/reports/` directory doesn't exist. CLI doesn't auto-create it. | Manual fix: `mkdir -p .taskmaster/reports`. Template init-project.sh should create this directory. |
| F8 | 3.3   | INFO     | CLI `parse-prd` runs twice (duplicate output). Appears to be a CLI bug — same command runs the AI call twice. Wasted ~344k tokens on duplicate. | Cosmetic but wastes API tokens. task-master CLI issue. |
| F9 | 4.1   | LOW      | Task 1 (project setup) is complexity 2 scaffolding — not a meaningful TDD candidate. Test plan says "pick ONE task" but the first available task has no testable logic. | Implemented task 1 as setup, then did proper TDD on task 2 (SQLite storage). Better to have the test plan suggest picking a task with logic. |
| F10 | 4.4   | INFO     | pyproject.toml needed `[tool.setuptools] packages = []` and `[build-system]` for scripts-only project to install with `pip install -e .`. | Standard Python packaging requirement for non-package projects. Not a template issue. |
| F11 | T2-Step1 | MEDIUM | `.claude/settings.local.json` and `.claude/instincts/` are gitignored by default. Wiring hooks requires `git add -f` to track configuration, which is unintuitive for sharing hook setups across team. | Used `git add -f`. Template should document this or provide a `--track` flag in init. |
| F12 | T2-Step2 | INFO | Instructions say tasks.json is empty, but health-monitoring tag data persisted from Test 1 in `.taskmaster/tasks/tasks.json`. The root `.taskmaster/tasks.json` is empty but CLI uses the nested path. | Not a real issue — tag-based task data survives across sessions as designed. Skipped re-parsing. |
| F13 | T2-Step4 | LOW | `/learn` skill is purely instructional — it tells Claude *how* to extract patterns but has no automation. The actual extraction is manual JSON file creation. No validation of instinct schema. | Works but relies entirely on Claude following the skill instructions correctly. A schema validator or helper script would reduce errors. |
| F14 | T2-Step5 | LOW | `/instinct-status` skill is display-only instructions with no actual rendering engine. Output format depends on Claude formatting it correctly each time. Inconsistent rendering across sessions. | Acceptable for MVP. A CLI tool (`task-master instinct-status`) would be more reliable. |
| F15 | T2-Step6 | LOW | `/instinct-export` default behavior exports only active instincts (>0.7 confidence). With all instincts at candidate level (0.4-0.5), default export would produce empty file. Had to override filter for testing. | Skill should have a `--all` or `--include-candidates` flag. Default is too restrictive for early-stage projects. |
| F16 | T2-Step7 | INFO | `/evolve` correctly reports no clusters — 4 instincts across 4 categories, none at active confidence. This is expected but means evolution can't be meaningfully tested in a single session. | By design — evolution requires multiple sessions of reinforcement. Multi-session testing needed. |
| F17 | T2-Step8 | INFO | Stop hooks can't be verified within the session — they only fire on session end. Verification is limited to checking the JSON config, not actual execution. | Need to check `.claude/sessions/` and `.claude/instincts/candidates/` after session restart to confirm hooks fired. |
| F18 | T3-Step1 | INFO | `session-end.sh` confirmed not firing — `.claude/sessions/` directory still doesn't exist in Test 3. Session summaries are not persisting across sessions. | Known from Test 2. pattern-extraction.sh DID fire (4 candidates present). Session persistence is partial. |
| F19 | T3-Step3 | LOW | `/orchestrate feature` is a skill that outputs instructions, not an automated pipeline. Each agent must be manually invoked via Task tool. No automatic sequencing, no retry evaluation loop, no progress tracking. | Followed the skill instructions manually. The skill is a recipe, not automation. A true orchestration engine would be more valuable. |
| F20 | T3-Step3 | LOW | TDD guide agent wrote test file with unused imports (logging, dataclass, call, pytest) — 4 lint warnings in generated code. Had to clean up after the agent. | Agent-generated code should pass basic lint checks. Consider adding a lint step to the TDD guide skill output. |
| F21 | T3-Step3 | MEDIUM | TDD guide agent's tests expected `ConsoleAlertChannel` to use f-string formatting, while the planner agent specified `%s`-style logging. This interface mismatch caused 3 test failures requiring implementation adjustment. | Agent handoffs need better interface contracts. The test agent's expectations should be the source of truth (since TDD), but inconsistency between planner and tdd-guide creates friction. |
| F22 | T3-Step4 | LOW | `/eval define` is a skill with instructions, not a tool. Must manually create the `.claude/evals/` directory and write the eval file. No schema validation for eval definitions. | Works but manual. Would benefit from a CLI tool that scaffolds the eval definition with prompts. |
| F23 | T3-Step5 | LOW | `/eval check` requires manually running each test subset and mapping results to eval criteria. No automation for linking tests to eval items. | Functional but labor-intensive. A test tag/marker system linking tests to eval criteria would automate this. |
| F24 | T3-Step6 | INFO | `/eval metrics` relies on optional tools (mypy, radon, bandit, pytest-cov). In this project 3/4 metric tools are missing, producing mostly SKIP results. | Expected for a lightweight project. The metrics command handles missing tools gracefully with SKIP. |
